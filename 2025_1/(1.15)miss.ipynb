{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPv9aHiZLXbPeyhnxj5zOx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Keita062/work/blob/main/(1_15)miss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6da1WhCrrrH1",
        "outputId": "b24e9807-0e73-4e72-b892-828ae6b89d43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/232.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (11.0.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.5 pypdfium2-4.30.1\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.9.4)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from wordcloud) (1.26.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud) (11.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from wordcloud) (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2\n",
        "!pip install pdfplumber\n",
        "!pip install nltk\n",
        "!pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_paths = [\n",
        "    \"/content/A Soft Sensor Method with Uncertainty-Awareness and Self-Explanation Based on Large Language Models Enhanced by Domain Knowledge Retrieval.pdf\",\n",
        "    \"/content/AI-Driven Diabetic Retinopathy Screening Multicentric Validation of AIDRSS in India.pdf\",\n",
        "    \"/content/CONTINUUM Detecting APT Attacks through Spatial-Temporal Graph Neural Networks.pdf\",\n",
        "    \"/content/Constraints as Rewards Reinforcement Learning for Robots without Reward Functions.pdf\",\n",
        "    \"/content/DPO Kernels  A Semantically-Aware, Kernel-Enhanced, and Divergence-Rich Paradigm for Direct Preference Optimization.pdf\",\n",
        "    \"/content/DiReCT Diagnostic Reasoning for Clinical Notes via Large Language Models.pdf\",\n",
        "    \"/content/Exploring Gradient Subspaces Addressing and Overcoming LoRA's Limitations in Federated Fine-Tuning of Large Language Models.pdf\",\n",
        "    \"/content/Hyperbolic Contrastive Learning for Hierarchical 3D Point Cloud Embedding.pdf\",\n",
        "    \"/content/INFELM In-depth Fairness Evaluation of Large Text-To-Image Models.pdf\",\n",
        "    \"/content/LightGNN Simple Graph Neural Network for Recommendation.pdf\",\n",
        "    \"/content/Lived Experience Not Found LLMs Struggle to Align with Experts on Addressing Adverse Drug Reactions from Psychiatric Medication Use.pdf\",\n",
        "    \"/content/Mathematical Definition and Systematization of Puzzle Rules.pdf\",\n",
        "    \"/content/MedCoDi-M A Multi-Prompt Foundation Model for Multimodal Medical Data Generation.pdf\",\n",
        "    \"/content/Migician Revealing the Magic of Free-Form Multi-Image Grounding in Multimodal Large Language Models.pdf\",\n",
        "    \"/content/Model Checking in Medical Imaging for Tumor Detection and Segmentation.pdf\",\n",
        "    \"/content/More is not always better Enhancing Many-Shot In-Context Learning with Differentiated and Reweighting Objectives.pdf\",\n",
        "    \"/content/Neural Network Prediction of Strong Lensing Systems with Domain Adaptation and Uncertainty Quantification.pdf\",\n",
        "    \"/content/PRMBench A Fine-grained and Challenging Benchmark for Process-Level Reward Models.pdf\",\n",
        "    \"/content/Piano Transcription by Hierarchical Language Modeling with Pretrained Roll-based Encoders.pdf\",\n",
        "    \"/content/Rethinking Adversarial Attacks in Reinforcement Learning from Policy Distribution Perspective.pdf\",\n",
        "    \"/content/Samba-ASR State-Of-The-Art Speech Recognition Leveraging Structured State-Space Models.pdf\",\n",
        "    \"/content/SenseRAG Constructing Environmental Knowledge Bases with Proactive Querying for LLM-Based Autonomous Driving.pdf\",\n",
        "    \"/content/Socratic Questioning Learn to Self-guide Multimodal Reasoning in the Wild.pdf\",\n",
        "    \"/content/The Race to Efficiency A New Perspective on AI Scaling Laws.pdf\",\n",
        "]"
      ],
      "metadata": {
        "id": "w7zQWJNXwAF-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import PyPDF2\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.decomposition import PCA\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "EAmLTu1v8DNZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTKのリソースをダウンロード\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ms3pmBH1AqlJ",
        "outputId": "3a26421c-b83e-4062-ebdf-e2b41b852845"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ストップワードを削除する関数\n",
        "def remove_stopwords(text):\n",
        "    stop_words = stopwords.words(\"english\")\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text"
      ],
      "metadata": {
        "id": "VPnytEwUA0SX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PDF処理の関数\n",
        "def process_pdf(pdf_path):\n",
        "    try:\n",
        "        with open(pdf_path, \"rb\") as pdf_file:\n",
        "            reader = PyPDF2.PdfReader(pdf_file)\n",
        "            text = \"\"\n",
        "            for page in reader.pages:\n",
        "                text += page.extract_text()\n",
        "\n",
        "        # テキストのクリーンアップ\n",
        "        cleaned_text = re.sub(r\"\\s+\", \" \", text)\n",
        "        cleaned_text = re.sub(r\"[^a-zA-Z0-9ぁ-んァ-ン一-龥]\", \" \", cleaned_text)\n",
        "\n",
        "        # ストップワードを削除\n",
        "        cleaned_text = remove_stopwords(cleaned_text)\n",
        "\n",
        "        # 単語と文のトークン化\n",
        "        words = word_tokenize(cleaned_text)\n",
        "        sentences = sent_tokenize(cleaned_text)\n",
        "\n",
        "        # 結果を返す\n",
        "        return cleaned_text, words, sentences\n",
        "    except Exception as e:\n",
        "        print(f\"{pdf_path}の処理中にエラーが発生しました: {e}\")\n",
        "        return None, None, None\n"
      ],
      "metadata": {
        "id": "tnHexLRYA54d"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PDFごとに処理を実行\n",
        "for pdf_path in pdf_paths:\n",
        "    cleaned_text, words, sentences = process_pdf(pdf_path)\n",
        "    if cleaned_text is not None:\n",
        "        print(f\"処理されたPDF: {pdf_path}\")\n",
        "        print(f\"クリーンアップ後のテキスト（最初の500文字）: {cleaned_text[:500]}\")\n",
        "        print(f\"単語トークン化（最初の10単語）: {words[:10]}\")\n",
        "        print(f\"文トークン化（最初の2文）: {sentences[:2]}\")\n",
        "        print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8egibR3RA_6b",
        "outputId": "7d862f83-8dc0-4509-ee7e-9d23ade4c9c8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/A Soft Sensor Method with Uncertainty-Awareness and Self-Explanation Based on Large Language Models Enhanced by Domain Knowledge Retrieval.pdfの処理中にエラーが発生しました: No such file or directory: '/root/nltk_data/corpora/stopwords/english'\n",
            "/content/AI-Driven Diabetic Retinopathy Screening Multicentric Validation of AIDRSS in India.pdfの処理中にエラーが発生しました: No such file or directory: '/root/nltk_data/corpora/stopwords/english'\n",
            "/content/CONTINUUM Detecting APT Attacks through Spatial-Temporal Graph Neural Networks.pdfの処理中にエラーが発生しました: No such file or directory: '/root/nltk_data/corpora/stopwords/english'\n",
            "/content/Constraints as Rewards Reinforcement Learning for Robots without Reward Functions.pdfの処理中にエラーが発生しました: No such file or directory: '/root/nltk_data/corpora/stopwords/english'\n",
            "/content/DPO Kernels  A Semantically-Aware, Kernel-Enhanced, and Divergence-Rich Paradigm for Direct Preference Optimization.pdfの処理中にエラーが発生しました: No such file or directory: '/root/nltk_data/corpora/stopwords/english'\n",
            "/content/DiReCT Diagnostic Reasoning for Clinical Notes via Large Language Models.pdfの処理中にエラーが発生しました: No such file or directory: '/root/nltk_data/corpora/stopwords/english'\n",
            "/content/Exploring Gradient Subspaces Addressing and Overcoming LoRA's Limitations in Federated Fine-Tuning of Large Language Models.pdfの処理中にエラーが発生しました: No such file or directory: '/root/nltk_data/corpora/stopwords/english'\n",
            "/content/Hyperbolic Contrastive Learning for Hierarchical 3D Point Cloud Embedding.pdfの処理中にエラーが発生しました: No such file or directory: '/root/nltk_data/corpora/stopwords/english'\n",
            "/content/INFELM In-depth Fairness Evaluation of Large Text-To-Image Models.pdfの処理中にエラーが発生しました: No such file or directory: '/root/nltk_data/corpora/stopwords/english'\n",
            "/content/LightGNN Simple Graph Neural Network for Recommendation.pdfの処理中にエラーが発生しました: No such file or directory: '/root/nltk_data/corpora/stopwords/english'\n",
            "/content/Lived Experience Not Found LLMs Struggle to Align with Experts on Addressing Adverse Drug Reactions from Psychiatric Medication Use.pdfの処理中にエラーが発生しました: No such file or directory: '/root/nltk_data/corpora/stopwords/english'\n",
            "/content/Mathematical Definition and Systematization of Puzzle Rules.pdfの処理中にエラーが発生しました: No such file or directory: '/root/nltk_data/corpora/stopwords/english'\n",
            "/content/MedCoDi-M A Multi-Prompt Foundation Model for Multimodal Medical Data Generation.pdfの処理中にエラーが発生しました: No such file or directory: '/root/nltk_data/corpora/stopwords/english'\n",
            "/content/Migician Revealing the Magic of Free-Form Multi-Image Grounding in Multimodal Large Language Models.pdfの処理中にエラーが発生しました: No such file or directory: '/root/nltk_data/corpora/stopwords/english'\n",
            "/content/Model Checking in Medical Imaging for Tumor Detection and Segmentation.pdfの処理中にエラーが発生しました: No such file or directory: '/root/nltk_data/corpora/stopwords/english'\n",
            "/content/More is not always better Enhancing Many-Shot In-Context Learning with Differentiated and Reweighting Objectives.pdfの処理中にエラーが発生しました: No such file or directory: '/root/nltk_data/corpora/stopwords/english'\n",
            "/content/Neural Network Prediction of Strong Lensing Systems with Domain Adaptation and Uncertainty Quantification.pdfの処理中にエラーが発生しました: No such file or directory: '/root/nltk_data/corpora/stopwords/english'\n",
            "/content/PRMBench A Fine-grained and Challenging Benchmark for Process-Level Reward Models.pdfの処理中にエラーが発生しました: No such file or directory: '/root/nltk_data/corpora/stopwords/english'\n",
            "/content/Piano Transcription by Hierarchical Language Modeling with Pretrained Roll-based Encoders.pdfの処理中にエラーが発生しました: No such file or directory: '/root/nltk_data/corpora/stopwords/english'\n",
            "/content/Rethinking Adversarial Attacks in Reinforcement Learning from Policy Distribution Perspective.pdfの処理中にエラーが発生しました: No such file or directory: '/root/nltk_data/corpora/stopwords/english'\n",
            "/content/Samba-ASR State-Of-The-Art Speech Recognition Leveraging Structured State-Space Models.pdfの処理中にエラーが発生しました: No such file or directory: '/root/nltk_data/corpora/stopwords/english'\n",
            "/content/SenseRAG Constructing Environmental Knowledge Bases with Proactive Querying for LLM-Based Autonomous Driving.pdfの処理中にエラーが発生しました: No such file or directory: '/root/nltk_data/corpora/stopwords/english'\n",
            "/content/Socratic Questioning Learn to Self-guide Multimodal Reasoning in the Wild.pdfの処理中にエラーが発生しました: No such file or directory: '/root/nltk_data/corpora/stopwords/english'\n",
            "/content/The Race to Efficiency A New Perspective on AI Scaling Laws.pdfの処理中にエラーが発生しました: No such file or directory: '/root/nltk_data/corpora/stopwords/english'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2MbvGVD0BD0w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
